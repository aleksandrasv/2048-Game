{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aleksandrasv/2048-Game/blob/master/Lab_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMM2e3OP6Kn4"
      },
      "source": [
        "# **Lab 3: Logistic Regression and Decision Tree**\n",
        "\n",
        "CS 412, Introduction to Machine Learning\n",
        "\n",
        "Department of Computer Science, University of Illinois at Chicago\n",
        "\n",
        "***This is a group work for at most four students.***\n",
        "\n",
        "This is your third lab work, and you will work on it with your teammates. You will learn how to apply the logistic regression model to recognize images of hand-written digits. You will also learn how to build a decision tree to visually and explicitly represent decision making. \n",
        "\n",
        "***Deadline:***\n",
        "This assignment is due **April 12** (Anywhere on Earth, [AoE](https://www.timeanddate.com/time/zones/aoe)). That is, you can resubmit as often as you like provided that anywhere on Earth is still on or before this date. \n",
        "\n",
        "***How to submit:***\n",
        "See bottom of the page\n",
        "\n",
        "***Python version:***\n",
        "The code should work on Python 3.7 or later, though it might work on earlier versions (not tested). There should be no version problem if you work on Colab.  If you use Colab, ignore the following message when you open the notebook (if it shows up): \\\\\n",
        "`Unrecognized runtime \"python_defaultSpec_1600651579462\"; defaulting to \"python3\"`\n",
        "\n",
        "See a more detailed introduction to Python and Colab at this [link](https://colab.research.google.com/github/cs231n/cs231n.github.io/blob/master/python-colab.ipynb#scrollTo=nxvEkGXPM3Xh).  You may also find the setup instructions useful [here](https://cs231n.github.io/setup-instructions/).  Here are summaries that compare Matlab with Numpy: [link-1](https://numpy.org/doc/stable/user/numpy-for-matlab-users.html), [link-2](https://realpython.com/matlab-vs-python/).\n",
        "\n",
        "**Please note before starting the lab:**\n",
        "\n",
        "1. If you use Colab, copy this file to your own Google Drive so that you can edit it or share it with your teammates.\n",
        "\n",
        "2. Since the experiments involve randomness, it is important to ensure that your results are replicable. To this end, your implementation should take one integer (or any numeric value) as a seed that is used to initialize the random number generators.\n",
        "See, e.g. [random.seed](https://docs.python.org/3/library/random.html).\n",
        "This has been done for you in the first code block below.\n",
        "\n",
        "3. No auto-grader will be used or provided.  Instead, <font color='red'> there are unit test cases provided after each function you need to implement. Make good use of them.</font>  In the logistic regression part, it is fine if your result is within 1% relative difference from the reference result.  In the decision tree part, your result should be exactly the same as the reference result, except the `gini_score` function where 1% relative difference will be fine.\n",
        "\n",
        "4. The label $y^i$ used in this lab corresponds to $r^t$ in the lecture slides.  It is far more common to denote the ground truth label by $y$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2jeOJg80h6N"
      },
      "source": [
        "# Let's first import some modules for this experiment\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS5vLRbndy7h"
      },
      "source": [
        "#Problem 1: Logistic Regression {-}\n",
        "\n",
        "In this problem, we will implement a Logistic Regression model with gradient descent from scratch. Logistic regression is a statistical model used for binary classification. We start with the fundamental mathematics and statistics behind logistic regression, and then extend it to multinomial logistic regression which can handle multi-class classification problems. To help you fully understand how they work, we will be working on a real dataset to recognize images of hand-written digits.\n",
        "\n",
        "## 1.1 From linear regression to logistic regression {-}\n",
        "\n",
        "In our previous lab assignment, we have learned how to use linear regression to predict the quality of wines. Actually linear regression is one of the most extensively used statistical technique for predictive modelling analysis thanks to its simplicity. Let us take a quick review of this method.\n",
        "\n",
        "### 1.1.1 Recap of linear regression {-}\n",
        "\n",
        "Linear regression assumes that the dependence of the target $y$ on the features $x_1, ..., x_m$ is linear, even if the true regression function is nonlinear. One benefit of making a linear dependence assumption is that the relationship between the target and features can be easily interpreted.\n",
        "\n",
        "Let's define $f_w(x)$ as the hypothesis for $y$ as a function of $x\\in\\mathbb{R}^m$, under the weight vector $w\\in\\mathbb{R}^m$.  This results in the following prediction function:\n",
        "$$f_w(x) = x^Tw.$$\n",
        "Our goal is to find the optimal $w$ that maps $f_w(x)$ to $y$ as accurately as possible. To achieve that, we use gradient descent to minimize the squared loss as the cost function:\n",
        "$$L(w) = \\frac{1}{2}||x^Tw - y||^2.$$\n",
        "Once we have learned the optimal $w$ from training data, we can use the learned model to predict the real value for test examples. \n",
        "\n",
        "In essence, linear regression is predicting continuous variables instead of binary variables. Then a natural question is whether linear regression can be used to solve classification problems. The answer is affirmative. Considering a binary classification problem, one can set up a threshold to distinguish different categories. Say if the predicted continuous value is greater than the threshold value, the data point will be classified as positive. Otherwise, it will be classified as negative. However, these predictions are not sensible for classification because the predicted values range from $-\\infty$ to $\\infty$ which can lead to adverse consequences in real-time. As a result, logistic regression comes to play.\n",
        "\n",
        "### 1.1.2 Logistic Regression - Modeling **(4 points)** {-}\n",
        "\n",
        "The basic idea of logistic regression is to produce probabilities out of linear regression. To achieve this, it feeds the learned score $x^Tw$ into a non-linear transformation, which is known as a sigmoid function:\n",
        "$$\\sigma(z) = \\frac{1}{1+e^{-z}}.$$\n",
        "Note that the sigmoid function $\\sigma(z)$ transforms an unbounded real number $z$ into the interval [0,1]:\n",
        "$$\n",
        "\\begin{align}\n",
        "&\\sigma(z) \\rightarrow 1, \\quad as \\quad z \\rightarrow \\infty\\\\\n",
        "&\\sigma(0) = \\frac{1}{2}, \\\\\n",
        "&\\sigma(z) \\rightarrow 0, \\quad as \\quad  z \\rightarrow -\\infty.\n",
        "\\end{align}\n",
        "$$\n",
        "Moreover, $\\sigma(z)$ is differentiable and its derivative has a nice property\n",
        "for convenient computation\n",
        "\\begin{align}\n",
        "\\sigma'(z) &= -\\frac{1}{(1+e^{-z})^2}(-e^{-z}) \\\\\n",
        "&= \\frac{1}{1+e^{-z}}\\frac{e^{-z}}{1+e^{-z}} \\\\\n",
        "&= \\frac{1}{1+e^{-z}}(1 - \\frac{1}{1+e^{-z}}) \\\\\n",
        "&=\\sigma(z)(1-\\sigma(z)).\n",
        "\\end{align}\n",
        "\n",
        "As we can see, if we modify $f_w$ to\n",
        "$$f_w(x) = \\sigma(x^Tw),$$\n",
        "then we have a model that outputs probabilities of an example $x$ belonging to the positive class, or in a mathematical form:\n",
        "$$P(y=1|x;w) = \\frac{1}{1+e^{-x^Tw}}$$\n",
        "For the negative class we have\n",
        "$$P(y=0|x;w) = \\frac{e^{-x^Tw}}{1+e^{-x^Tw}} = 1 - P(y=1|x;w)$$\n",
        "At training time, we learn the value of $w$ to yield high values for $P(y=1|x;w)$ when $x$ is a positive example, and to yield low values for $P(y=0|x;w)$ when $x$ is a negative example. \n",
        "\n",
        "In practice, a real dataset contains many training examples. To make the computation efficient, in this experiment, we will process all data points at once instead of one at a time. Let's assume the dataset contains $n$ examples  which allows us to assemble a feature matrix $X =[x^1, x^2, ..., x^n]^T \\in\\mathbb{R}^{n\\times m}$, \n",
        "where $x^i$ represents the $i$-th training example, and $^T$ is matrix transpose.\n",
        "Then the prediction can be written as\n",
        "$$\\begin{pmatrix} P(y=1|x^1;w) \\\\ \\vdots \\\\ P(y=1|x^n; w)\\end{pmatrix} = \\frac{1}{1+e^{-Xw}},$$\n",
        "where $Xw$ leads to an $n$-dimensional vector,\n",
        "and all other operations on the right-hand side (e.g., exponentiation and reciprocal) are performed elementwise on a vector.\n",
        "\n",
        "In the following code block, implement the functions `sigmoid` and `logistic_regression` that can handle *batch* inputs (see the header of the functions and the unit tests)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRJDVn2lF7Pv"
      },
      "source": [
        "def sigmoid(z):\n",
        "  \"\"\"\n",
        "  sigmoid function that maps inputs into the interval [0,1]\n",
        "  Your implementation must be able to handle the case when z is a vector (see unit test)\n",
        "  Inputs:\n",
        "  - z: a scalar (real number) or a vector\n",
        "  Outputs:\n",
        "  - trans_z: the same shape as z, with sigmoid applied to each element of z\n",
        "  \"\"\"\n",
        "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "  return trans_z\n",
        "\n",
        "def logistic_regression(X, w):\n",
        "  \"\"\"\n",
        "  logistic regression model that outputs probabilities of positive examples\n",
        "  Inputs:\n",
        "  - X: an array of shape (num_sample, num_features)\n",
        "  - w: an array of shape (num_features,)\n",
        "  Outputs:\n",
        "  - logits: a vector of shape (num_samples,)\n",
        "  \"\"\"\n",
        "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "  return logits\n",
        "\n",
        "# unit test\n",
        "# sample inputs:\n",
        "# z = np.array([215, -108, 0, 0.32])\n",
        "# X = np.array([[4.17022005e-01, 7.20324493e-01, 1.14374817e-04],\n",
        "#               [3.02332573e-01, 1.46755891e-01, 9.23385948e-02],\n",
        "#               [1.86260211e-01, 3.45560727e-01, 3.96767474e-01],\n",
        "#               [5.38816734e-01, 4.19194514e-01, 6.85219500e-01]])\n",
        "# w = np.array([0.20445225, 0.87811744, 0.02738759])\n",
        "\n",
        "# sample outputs:\n",
        "# out1 = sigmoid(z)\n",
        "# out1 : [1.00000000e+00 1.24794646e-47 5.00000000e-01 5.79324252e-01]\n",
        "# out2 = logistic_regression(X, w)\n",
        "# out2 : [0.67212099 0.5481529  0.5871972  0.62176131]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUWyMHakF7ve"
      },
      "source": [
        "### 1.1.3 Loss function **(8 points)** {-}\n",
        "Recall that in linear regression, we optimize the model by minimizing the square loss:\n",
        "$$L(w) = \\frac{1}{2}||x^Tw - y||^2$$\n",
        "This loss is a convex function w.r.t $w$, hence the local minimum is also the global minimum.\n",
        "\n",
        "A naive way to extend linear regression to classification would be to use the loss $\\frac{1}{2}||\\sigma(x^Tw) - y||^2$, where $y$ is either 1 or 0 (for positive or negative, respectively).  This loss turns out very hard to optimize with an algorithm like gradient descent, because the loss function is not convex in $w$. In other words, there can be more than one local minimum and we wouldn't be assured to find the global minimum that best optimizes the loss.\n",
        "\n",
        "Instead of minimizing the square error as in the linear regression, we can resort to maximizing the likelihood of the training set as in many other machine learning algorithms. By making the standard assumption that training examples are generated independently, the likelihood function is given by\n",
        "$$\n",
        "\\begin{align}\n",
        "L(w) &= P(y^1, ..., y^n|x^1, ..., x^n; w) \\\\\n",
        "&= \\prod_{i=1}^n P(y^i|x^i;w)\\\\\n",
        "&= \\prod_{i=1}^n (\\sigma(w^T x^i))^{y^i}(1-\\sigma(w^T x^i))^{1-y^i}.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "To see the last step, just enumerate the two cases of $y^i = 1$ or $0$ since we are considering a binary classification problem.\n",
        "To simplify the computation, let us maximize the logarithm of the likelihood,\n",
        "which is equivalent to minimizing $-\\frac{1}{n}$ times the log-likelihood:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\ell(w) &= -\\frac{1}{n}\\log \\prod_{i=1}^n (\\sigma(w^T x^i))^{y^i}(1-\\sigma(w^T x^i))^{1-y^i} \\\\\n",
        "\\tag{1}\n",
        "&= \\frac{1}{n}\\sum_{i=1}^n \\left[-{y^i}\\log(\\sigma(w^T x^i)) - (1-y^i)\\log(1-\\sigma(w^T x^i))\\right].\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "The term inside the square bracket is generally referred to as cross-entropy loss, or logistic loss.\n",
        "That is, for a given data point $\\{x, y\\}$,\n",
        "it can be written as two cases: \n",
        "\\begin{align}\n",
        "\\ell(x; w) &= \n",
        "\\tag{2}\n",
        "-{y}\\log(\\sigma(w^T x)) - (1-y)\\log(1-\\sigma(w^T x)) \\\\\n",
        "\\tag{3}\n",
        "&= \\begin{cases}\n",
        "-\\log(1-\\sigma(x^Tw)) & \\text{if } y=0\\\\\n",
        "-\\log(\\sigma(x^Tw))    & \\text{if } y=1\\\\\n",
        "\\end{cases}.\n",
        "\\end{align}\n",
        "\n",
        "In practice, we surely implement Eq 3 based on the value of $y$.\n",
        "Eq 2, however, gives more convenience in mathematical derivation as it unifies two cases neatly.\n",
        "If we plot the curve of $\\ell(x; w)$ as a function of $w$, we will see it is a convex function and therefore the gradient descent algorithm can find its global minima. Just like in linear regression, we will use the derivative of the loss function to calculate a gradient descent step. Please derive \n",
        "\n",
        "1. The gradient of $\\ell(x; w)$ in Eq 2 with respect to (w.r.t.) $w$. \n",
        "<font color='red'> Fill your solution in the following line </font>: \n",
        "\n",
        "$$\\nabla_w\\ell(x;) = ??$$ \n",
        "\n",
        "\n",
        "2. the gradient of $\\ell(w)$ in Eq 1 w.r.t. $w$.  For computational efficiency, we would like to express it in terms of two the feature matrix $X = (x^1, x^2, ..., x^n)^T \\in\\mathbb{R}^{n\\times m}$ and the label vector $Y = (y^1, y^2, ..., y^n)\\in\\mathbb{R}^n$).  <font color='red'> Fill your solution in the following line </font>:\n",
        "\n",
        "$$\\nabla_w\\ell(w) = ??$$ \n",
        "\n",
        "Then, implement a function `logistic_loss` that computes $\\ell(w)$ and the gradient in $w$.  You are strongly recommended to use the matrix/vector implementation as introduced in the class, as opposed to Figure 10.6 of the textbook.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyj_fVzBuo03"
      },
      "source": [
        "def logistic_loss(X, w, y):\n",
        "  \"\"\"\n",
        "  a function that compute the loss value for the given dataset (X, y) and parameter w;\n",
        "  It also returns the gradient of loss function w.r.t w\n",
        "  Here (X, y) can be a set of examples, not just one example.\n",
        "  Inputs:\n",
        "  - X: an array of shape (num_sample, num_features)\n",
        "  - w: an array of shape (num_features,)\n",
        "  - y: an array of shape (num_sample,), it is the ground truth label of data X\n",
        "  Output:\n",
        "  - loss: a scalar which is the value of loss function for the given data and parameters\n",
        "  - grad: an array of shape (num_featues,), the gradient of loss \n",
        "  \"\"\"\n",
        "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "  return loss, grad\n",
        "#unit test\n",
        "# inputs:\n",
        "# X = np.array([[0.67046751, 0.41730480, 0.55868983],\n",
        "#               [0.14038694, 0.19810149, 0.80074457],\n",
        "#               [0.96826158, 0.31342418, 0.69232262],\n",
        "#               [0.87638915, 0.89460666, 0.08504421]])\n",
        "# w = np.array([0.03905478, 0.16983042, 0.8781425 ])\n",
        "# Y = np.array([1, 1, 0, 1])\n",
        "\n",
        "# sample outputs:\n",
        "# loss, grad = logistic_loss(X, w, Y)\n",
        "# loss: 0.626238298577102\n",
        "# grad: [-0.00483685, -0.09821878, -0.0080873 ]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgDSd4ABKUM3"
      },
      "source": [
        "## 1.2 Recognizing hand-written digits with logistic regression {-}\n",
        "\n",
        "We have gone through all the theoretical concepts of the logistic regression model. It's time to put hands on a real problem in which we aim to recognize images of hand-written digits. The dataset we will use is the Optical Recognition of Handwritten Digits dataset, and the description of the dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits). \n",
        "\n",
        "### 1.2.1 Data preprocessing (not for grading){-}\n",
        "The original dataset contains 10 classes (digits 0 to 9). Since for now we are concerned about logistic regression for binary classification, we will only use a subset of the dataset that contains 360 examples from 2 classes (digits 0 and 1).  Each example is a $8\\times 8$ matrix (image) where each element is an integer in the range $[0,16]$. Let's load the dataset by using the off-the-shell method from `sklearn` and print out some images to get a good understanding of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AX98obkFPLh"
      },
      "source": [
        "# set up the code for this experiment\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "%matplotlib inline\n",
        "np.random.seed(1)\n",
        "\n",
        "# load the digits dataset\n",
        "digits = load_digits(n_class=2)\n",
        "# digits is a dictionary-like object that hold all the features and labels,\n",
        "# along with some metadata about the dataset. \n",
        "# The features are stored in the '.data' member, a (#sample, #feature) array. \n",
        "# The labels are stored in the '.target' member.\n",
        "\n",
        "print(f'There are {len(digits.target)} examples in total.')\n",
        "print(f'All examples are images of hand-written digit {list(set(digits.target))[0]} \\\\\n",
        "        or hand-written digits {list(set(digits.target))[1]}')\n",
        "print(f'Each example is an array of shape {digits.data[0].shape}')\n",
        "print(f'An example of data point:\\n{digits.data[0]}')\n",
        "\n",
        "# You may wondering why the shape of data is (64,) instead of (8, 8). Actually,\n",
        "# You can access to matrix shape of data through the '.images' member.\n",
        "print(f'The shape of image is {digits.images[0].shape}') \n",
        "print(f'An example of 2D array data:\\n {digits.images[0]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kazXeCBCXLee"
      },
      "source": [
        "# The data we are interested in is made up of 8x8 images of digits. \n",
        "# Let's have a look at the first 6 images that are drawn from the dataset. \n",
        "# For these images, \n",
        "#   we know the digit they represented is given in the 'target' of the dataset.\n",
        "_, axes = plt.subplots(1, 6)\n",
        "img_label = list(zip(digits.images, digits.target))\n",
        "for ax, (img, target) in zip(axes, img_label[:6]):\n",
        "  ax.set_axis_off()\n",
        "  ax.imshow(img, cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "  ax.set_title('Label: %i' % target)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oveqe9GK526A"
      },
      "source": [
        "### 1.2.2 Padding features (not for grading) {-}\n",
        "As we did in Lab 2, to simplify the notation, we pad the input $x$ by inserting 1 to the **beginning** so that we can absorb the bias term into the parameter $w$.\n",
        "\n",
        "The following code morphs the variable `digits.data` by concatenating 1 and features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h4w6JIZ8Cpk"
      },
      "source": [
        "ones = np.ones(digits.data.shape[0]).reshape(-1, 1)\n",
        "digits.data = np.concatenate((ones, digits.data), axis=1)\n",
        "print(digits.data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSwnaHBAoQbt"
      },
      "source": [
        "### 1.2.3 Create training and test sets (not for grading) {-}\n",
        "As we have practiced in our previous lab assignment, we will use the `train_test_split()` method to partition the dataset into training and test sets. In this experiment, we use 80% data for training and the remaining 20% data for testing. To ensure your results are replicable, we will set the `random_state` argument of `train_test_split()` to **1**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y89isTi9qLcV"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, train_size=0.8, random_state=1)\n",
        "print(f'The training set contains {X_train.shape[0]} examples.')\n",
        "print(f'The testing set contains {X_test.shape[0]} examples.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n39CSTpbvpHR"
      },
      "source": [
        "### 1.2.3 Feature Normalization (not for grading) {-}\n",
        "In the previous lab assignment, we have implemented the function `featureNormalization()` to normalize the features that have different scale. In this lab, we will learn to use the built-in function `StandardScaler()` in `scikit-learn`. As we did in `featureNormalization()`, `StandardScaler()` returns standardized features by removing the mean and scaling to unit variance.\n",
        "Please read through the [API documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) for detailed instructions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJV3UYO1z4Zk"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "sc.fit(X_train)\n",
        "X_train = sc.transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u69oMti4KhzJ"
      },
      "source": [
        "### 1.2.5 Training the model with gradient descent **(13 points)**{-}\n",
        "Now after all the pre-processing, we can train a logisitic regression model with the training data.  It is quite straightforward to make predictions on test data by using the learned model. To simplify the task, when the probability of being positive is greater than 0.5, we classify the sample to 1. Otherwise, we classify it to 0.\n",
        "\n",
        "In this part, we will train the model with gradient descent. After that, predict the label for test examples and compute the test accuracy. You may want to follow the procedures below to obtain the results:\n",
        "+ Randomly initialize the parameter $w$ by `np.random.rand`.\n",
        "+ Use gradient descent to update $w$ (number of iteration `num_iters` and learning rate `lr` are provided).\n",
        "+ Plot the curve of the $\\ell(w)$ value as a function of how many update steps have been taken (you need a variable to store the history of $\\ell(w)$ values).\n",
        "+ Compute and report the test accuracy on test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYGPnvLAEvkO"
      },
      "source": [
        "num_iters = 200\n",
        "lr = 0.1\n",
        "\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "\n",
        "# training\n",
        "\n",
        "# plotting\n",
        "\n",
        "\n",
        "# testing\n",
        "\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrsS04vQm-Tj"
      },
      "source": [
        "## 1.3 Multinomial (multi-class) Logistic Regression (MLR) {-}\n",
        "\n",
        "So far we have built a logisitic regression model for binary classification. In this section, we aim to extend it to multinomial logistic regression for solving multi-class classification. More specifically, we expect the MLR model can predict one out of $k$ possible classes, where $k$ is the total number of classes. \n",
        "\n",
        "Recall that in binary logisitic regression, the output of the model is the probability of the positive class. Analogously, the MLR model should perform a series of mathematical operations to produce a vector encoding the probability that an example $x$ belongs to each class:\n",
        "\\begin{align}\n",
        "\\begin{pmatrix}\n",
        "  P(y=1|x; W) \\\\\n",
        "  \\vdots \\\\\n",
        "  p(y=k|x; W)\n",
        "\\end{pmatrix},\n",
        "\\quad where \\quad\n",
        "W = (w_1, \\ldots, w_k).\n",
        "\\end{align}\n",
        "\n",
        "Here $w_1, \\ldots, w_k$ are all $m$-dimensional vectors, one for each class.\n",
        "$W$ is an $m$-by-$k$ matrix.\n",
        "The class with the highest probability will be adopted as the prediction outcome for the given data $x$. \n",
        "Now the question is, how does the MLR model covert features to probability values? In binary logistic regression, we used the sigmoid function. \n",
        "In MLR, we can use the `softmax` to covert $(w_1^T x, \\ldots, w_k^T x)$ (which are often called logits) to probability values. \n",
        "For a $k$-class problem, this conversion formula is \n",
        "$$\n",
        "P(y=i|x; W) = \\frac{e^{w_i^Tx}}{\\sum_{j=1}^k e^{w_j^Tx}}.\n",
        "$$\n",
        "We will simply write\n",
        "\\begin{align}\n",
        "\\begin{pmatrix}\n",
        "  P(y=1|x; W) \\\\\n",
        "  \\vdots \\\\\n",
        "  p(y=k|x; W)\n",
        "\\end{pmatrix}\n",
        "=\n",
        "softmax (W^\\top x).\n",
        "\\end{align}\n",
        "To summarize, a weight vector $w_i$ is learned for each class,\n",
        "which produces $k$ logits $\\{w_i^T x\\}_{i=1}^k$ for each example. Then `softmax` is subsequently applied to these logits to derive the probabilities for different classes.\n",
        "\n",
        "**(4 points)** Please implement `softmax()` and `MLR()` functions in the following code block. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcrM2A3iO7qW"
      },
      "source": [
        "def softmax(x):\n",
        "  \"\"\"\n",
        "  Convert logits for each possible outcomes to probability values.\n",
        "  In this function, we assume the input x is a 2D matrix of shape (num_sample, num_classes).\n",
        "  So we need to normalize each row by applying the softmax function.\n",
        "  Inputs:\n",
        "  - x: an array of shape (num_sample, num_classse) which contains the logits for each input\n",
        "  Outputs:\n",
        "  - probability: an array of shape (num_sample, num_classes) which contains the\n",
        "                 probability values of each class for each input\n",
        "  \"\"\"\n",
        "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "  return probability\n",
        "\n",
        "def MLR(X, W):\n",
        "  \"\"\"\n",
        "  performs logistic regression on given inputs X\n",
        "  Inputs:\n",
        "  - X: an array of shape (num_sample, num_feature)\n",
        "  - W: an array of shape (num_feature, num_class)\n",
        "  Outputs:\n",
        "  - probability: an array of shape (num_sample, num_classes)\n",
        "  \"\"\"\n",
        "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "  return probability\n",
        "\n",
        "# unit test\n",
        "# sample inputs:\n",
        "# X = np.array([[0.49157316, 0.05336255, 0.57411761],\n",
        "#               [0.14672857, 0.58930554, 0.69975836],\n",
        "#               [0.10233443, 0.41405599, 0.69440016],\n",
        "#               [0.41417927, 0.04995346, 0.53589641],\n",
        "#               [0.66379465, 0.51488911, 0.94459476]])\n",
        "# W = np.array([[0.58655504, 0.90340192, 0.1374747, 0.13927635],\n",
        "#               [0.80739129, 0.39767684, 0.1653542, 0.92750858],\n",
        "#               [0.34776586, 0.75081210, 0.7259979, 0.88330609]])\n",
        "\n",
        "# sample outputs:\n",
        "# out1 = softmax(X)\n",
        "# out1: [[0.36613449 0.23622627 0.39763924]\n",
        "#        [0.23281662 0.36242881 0.40475457]\n",
        "#        [0.23960744 0.32724969 0.43314287]\n",
        "#        [0.35408647 0.24599602 0.39991751]\n",
        "#        [0.31388902 0.27046263 0.41564835]]\n",
        "# out2 = MLR(X, W)\n",
        "# out2: \n",
        "# [[0.22210723 0.32004009 0.21385397 0.24399871]\n",
        "#  [0.2278552  0.24858598 0.19040101 0.33315781]\n",
        "#  [0.21922197 0.25283567 0.20870744 0.31923492]\n",
        "#  [0.22296738 0.30913599 0.2195647  0.24833193]\n",
        "#  [0.22047099 0.32241683 0.16806773 0.28904445]]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYEhL39TR0jC"
      },
      "source": [
        "### 1.3.1 Cross entropy loss **(8 points)**{-}\n",
        "\n",
        "For the MLR model, generally, we use the cross-entropy loss which generalizes the the loss function we used in binary logistic regression\n",
        "$$\n",
        "\\ell(W) = -\\frac{1}{n}\\sum_{i=0}^n y_i^T\\log(p_i),\n",
        "\\quad \\text{where} \\quad  \n",
        "p_i =\n",
        "\\begin{pmatrix}\n",
        "P(y_i = 1 | x_i; W) \\\\\n",
        "\\vdots \\\\\n",
        "P(y_i = k | x_i; W) \n",
        "\\end{pmatrix}\n",
        "= softmax(W^T x_i).\n",
        "$$\n",
        "Here $p_i\\in\\mathbb{R}^k$ is a probabiltiy vector of sample $x_i$. Then we apply element-wise logarithm on $p_i$ to obtain $\\log(p_i)\\in\\mathbb{R}^k$.\n",
        "In addition, $y_i$ is a one-hot vector, where the component corresponding to the correct label is 1, and all the other components are 0. \n",
        "For instance, in a 5-class classification problem (say, digits 0-4), when the ground truth label for a data point is digit 3 (i.e., the fourth class because of the digit 0), we have $y = (0, 0, 0, 1, 0)^T$.\n",
        "As a result if $y_i$ encodes the fact that the true class for the $i$-th example is $c$, then $y_i^T\\log(p_i)$ simply returns\n",
        "$\\log P(y_i = c | x_i; W)$.\n",
        "\n",
        "Now we need to derive the gradient of $\\ell(W)$ w.r.t. $W$, \n",
        "and express it in terms of $X = (x_1, x_2, ..., x_n)^T \\in \\mathbb{R}^{n\\times m}$ and $Y=(y_1, y_2, ..., y_n)^T \\in\\mathbb{R}^{n\\times k}$. <font color='red'> Please fill you solution in the following line </font>\n",
        "$$\\nabla_W\\ell(W)= ??$$\n",
        "\n",
        "Recall that if $\\ell$ is a function that maps a matrix $W \\in \\mathbb{R}^{m \\times k}$ to a real number, then $\\nabla_W \\ell(W)$ is also an $m$-by-$k$ matrix, and its $(r,s)$-th element is $\\frac{\\partial \\ell(W)}{\\partial W_{rs}}$.\n",
        "\n",
        "Now implement the function `cross_entropy_loss` that returns $\\ell(W)$ and its gradient.   You are strongly recommended to use the matrix/vector implementation as introduced in the class, as opposed to Figure 10.8 of the textbook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLq-0JNwTq9I"
      },
      "source": [
        "def cross_entropy_loss(X, W, y):\n",
        "  \"\"\"\n",
        "  Inputs:\n",
        "  - X: an array of shape (num_sample, num_feature)\n",
        "  - W: an array of shape (num_feature, num_class)\n",
        "  - y: an array of shape (num_sample,)\n",
        "  Ouputs:\n",
        "  - loss: a scalar which is the value of loss function for the given data and parameters\n",
        "  - grad: an array of shape (num_featues, num_class), the gradient of the loss function \n",
        "  \"\"\"\n",
        "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "  return loss, grad\n",
        "\n",
        "# unit test\n",
        "# sample inputs:\n",
        "# X = np.array([[0.49157316, 0.05336255, 0.57411761],\n",
        "#               [0.14672857, 0.58930554, 0.69975836],\n",
        "#               [0.10233443, 0.41405599, 0.69440016],\n",
        "#               [0.41417927, 0.04995346, 0.53589641],\n",
        "#               [0.66379465, 0.51488911, 0.94459476]])\n",
        "# W = np.array([[0.58655504, 0.90340192, 0.1374747, 0.13927635],\n",
        "#               [0.80739129, 0.39767684, 0.1653542, 0.92750858],\n",
        "#               [0.34776586, 0.75081210, 0.7259979, 0.88330609]])\n",
        "# y = np.array([0, 1, 1, 0, 1])\n",
        "\n",
        "# sample outputs:\n",
        "# loss, grad = cross_entropy_loss(X, W, y)\n",
        "# loss:   1.3808433676397016\n",
        "# grad:[[-0.10040155, -0.07022596,  0.07138434,  0.09924316],\n",
        "#       [ 0.05164776, -0.21370799,  0.0615074 ,  0.10055283],\n",
        "#       [-0.06861677, -0.26705505,  0.13547167,  0.20020015]]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9r3WiknkyMW"
      },
      "source": [
        "### 1.3.2 Learning the model on real dataset **(13 points)**{-}\n",
        "In this last section, we will experiment on a subset of the hand-written digits dataset, and the task is a 10-class (also known as 10-way) classification. Compared with binary classification, the procedure of doing 10-ways classification is pretty much the same. Hence, in the following code block, you will need to train a MLR model and test it on test data. \n",
        "\n",
        "You can perform the following main steps to obtain the results:\n",
        "+ Load the whole dataset that contains 10 classes\n",
        "+ Normalize the features\n",
        "+ Create training and test sets (80% for training and 20% for testing)\n",
        "+ Randomly initialize the weight matrix $W$ by `np.random.rand` \n",
        "+ Update $W$ with gradient descent\n",
        "+ Plot the value of $\\ell(W)$ as a function of the number of gradient descent steps taken\n",
        "+ Predict the label for all test examples\n",
        "+ Report the test accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rejkRwJKkw3-"
      },
      "source": [
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "# Loading dataset\n",
        "\n",
        "# partition the data\n",
        "\n",
        "# normalize features\n",
        "\n",
        "# initialize W\n",
        "\n",
        "# updating W with gradient descent\n",
        "\n",
        "# plotting\n",
        "\n",
        "# testing\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX-TkM16D44y"
      },
      "source": [
        "# Porblem 2: Decision Tree {-}\n",
        "\n",
        "Decision trees are among the most powerful Machine Learning tools available today and are used in a wide variety of real-world applications. They are intuitive and easy to interpret. The final decision tree can explain exactly why a specific prediction was made, making it very attractive for operational use. In this section, we will explore how to implement a decision tree from scratch. \n",
        "\n",
        "The dataset we will use for this problem is the `Banknote` dataset. The task is to predict whether a banknote is authentic given a number of measures taken from a photograph. It is a binary classification problem. The dataset is stored in `data_banknote_authentication.csv` file, which contains 1372 samples(rows), 5 features (column 1-5) for each sample, and the last column (column 6) is the corresponding label.\n",
        "\n",
        "## 2.1 Implementing decision tree from scratch {-}\n",
        "\n",
        "In this lab, we only consider decision trees represented as a *binary* tree, i.e., each node can have \n",
        "\n",
        "1. no child, i.e., being a terminal node, \n",
        "2. two children that are both terminal nodes;\n",
        "3. one child being a terminal node and the other being an internal node (i.e., has its own children);\n",
        "4. two children which are both internal nodes.\n",
        "\n",
        "In general, it is possible to allow three or more children, but for simplicity, this lab only considers two children.\n",
        "\n",
        "A node represents a single input feature (attribute) and a split value on that feature, assuming the feature is numeric. The leaf nodes (we will call them as terminal nodes) contain an output class (0 or 1) which is used to make a prediction.\n",
        "Once created, a tree can be navigated for a new example by following the branching criteria in each internal node, until reaching a final prediction.\n",
        "\n",
        "Creating a binary decision tree is actually a process of dividing up the input space. A greedy approach is recursive binary splitting, where all the features and different split thresholds are tried using a cost function.\n",
        "The split with the best cost (lowest cost because we minimize cost) is selected. **All input features and all possible split thresholds are evaluated, and the combination of (feature, threshold) that minimizes the cost is chosen.** Then the dataset is divided into two subsets, and the same selection procedure is used in each of the two subsets, hence called **greedy**.\n",
        "\n",
        "Note we say \"threshold\" because all the features in the `banknote` dataset are numeric (i.e., continuously valued). When a feature is discrete, splitting can be done based on dividing all possible values into two categories, e.g.,\n",
        "{apple, orange} for the left, and {banana, kiwi, peach} for the right.\n",
        "\n",
        "In this lab, the **Gini cost function** is used which indicates how *pure* a node is.  A node's purity refers to the diversity of labels among all the training examples belonging to the node.\n",
        "Splitting continues until the number of training examples in a node falls below a threshold or **a maximum tree depth** is reached.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrIfL2Nh4iEw"
      },
      "source": [
        "### 2.1.1 Gini index (**7 points**) {-}\n",
        "\n",
        "This lab will use the Gini index as the cost function to evaluate splits.\n",
        "Every step of node construction requires splitting a datalist (a subset of training examples), which is based on one input feature and one value for that feature as the splitting threshold. It can be used to divide training examples into two groups of examples.  In particular, examples whose feature value is less than the threshold form the left group, and the rest examples form the right group.   \n",
        "\n",
        "A Gini score gives an idea of how good a split is by how mixed the classes are in the two groups created by the split. A perfect separation results in a Gini score of 0, where each group contains only one class of examples.\n",
        "In contrast, the worst case is when each group contains 50/50% of both classes, leading to a Gini score of 0.5 (for a 2 class problem). \n",
        "Assume we have $m$ groups of data after splitting ($m=2$ in this lab). The Gini index for group $j$ ($j = 1$ or $2$) can be expressed as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "g_j = 1-\\sum_{i=1}^nP_{ij}^2\n",
        "\\end{equation}\n",
        "where $P_{ij}$ is the probability of a sample being classified to class $i$. Specifically, it can be computed by counting:\n",
        "\n",
        "\\begin{equation}\n",
        "P_{ij} = \\frac{\\text{# examples of class i in group j}}{\\text{# examples in group j}}\n",
        "\\end{equation}\n",
        "The final Gini score can then be computed by weighted sum over all groups' Gini indices.\n",
        "\n",
        "\\begin{equation}\n",
        "G = \\sum_{j=1}^m w_jg_j, \\quad \\text{where} \\quad\n",
        "w_j = \\frac{\\text{# examples in group j}}{\\text{# examples in the datalist}}.\n",
        "\\end{equation}\n",
        "To better demonstrate the formula, let's go through an example step by step.\n",
        "Assume we have split the data into 2 groups:\n",
        "\n",
        "Group 1 contains **3** samples: **2** positive and **1** negative. \\\\\n",
        "Group 2 contains **4** samples: **2** positive and **2** negative. \\\\\n",
        "Then we can compute the Gini index for each group:\n",
        "\\begin{equation}\n",
        "g_1 = 1-\\left[(\\frac{1}{3})^2 + (\\frac{2}{3})^2\\right] = \\frac{4}{9} \\\\\n",
        "g_2 = 1-\\left[(\\frac{1}{2})^2 + (\\frac{1}{2})^2\\right] = \\frac{1}{2}\n",
        "\\end{equation}\n",
        "The final Gini score can be computed by :\n",
        "\\begin{equation}\n",
        "G = \\frac{3}{7}\\times g_1 + \\frac{4}{7}\\times g_2 = \\frac{10}{21}\n",
        "\\end{equation}\n",
        "\n",
        "In the following code block, impelment a function `gini_score` to compute the Gini score of two given groups. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwlOvbpQxFPE"
      },
      "source": [
        "def gini_score(groups, classes):\n",
        "  '''\n",
        "  Inputs: \n",
        "  groups: 2 lists of examples. Each example is a list, where the last element is the label.\n",
        "  classes: a list of different class labels (it's simply [0.0, 1.0] in this problem)\n",
        "  Outputs:\n",
        "  gini: gini score, a real number\n",
        "  '''\n",
        "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "  # count all samples at split point\n",
        "\n",
        "  # sum weighted Gini index for each group\n",
        "\n",
        "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "  return gini\n",
        "\n",
        "'''\n",
        "unit test:\n",
        "group1 = [[4.8, 3.1, 1],\n",
        "[5.4, 3.4, 1],\n",
        "[7.0, 3.2, 0],\n",
        "[6.4, 3.2, 0]]\n",
        "group2 = [[6.0, 3.0, 1],\n",
        "[5.0, 3.4, 1],\n",
        "[5.2, 3.5, 1]]\n",
        "classes = [0, 1]\n",
        "result = gini_score((group1, group2), classes)\n",
        "print(result)\n",
        "'''\n",
        "\n",
        "'''\n",
        "should print: 0.2857142857142857\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oxipdToxQTl"
      },
      "source": [
        "### 2.1.2 Create split (**3 points**) {-}\n",
        "\n",
        "Splitting a dataset means dividing a dataset into two lists of examples given  a feature and a splitting threshold for that feature.\n",
        "Once we have the two groups, we can then use the above Gini score function to evaluate the cost of the split.\n",
        "Splitting a dataset involves iterating over all examples, checking if its feature value is below or above the split threshold, and assigning the example to the left or right group respectively.\n",
        "\n",
        "In the following code block, implement a function `create_split`, which splits the given data list into two groups (left and right) for a given feature index and split threshold.  **Each `group` is nothing but a list of examples**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxrSUwiMzhXb"
      },
      "source": [
        "def create_split(index, threshold, datalist):\n",
        "  '''\n",
        "  Inputs:\n",
        "  index: The index of the feature used to split data. It starts from 0.\n",
        "  threshold: The threshold for the given feature based on which to split the data.\n",
        "        If an example's feature value is < threshold, then it goes to the left group.\n",
        "        Otherwise (>= threshold), it goes to the right group.\n",
        "  datalist: A list of samples. \n",
        "  Outputs:\n",
        "  left: List of samples\n",
        "  right: List of samples\n",
        "  '''\n",
        "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "  return left, right\n",
        "\n",
        "'''\n",
        "unit test:\n",
        "index = 1\n",
        "threshold = 3.4\n",
        "datalist = [[4.8, 3.1, 1.6, 1],\n",
        "[5.4, 3.4, 1.5, 1],\n",
        "[7.0, 3.2, 4.7, 0],\n",
        "[6.4, 3.6, 2.7, 0]]\n",
        "result = create_split(index, threshold, datalist)\n",
        "print(result)\n",
        "'''\n",
        "\n",
        "'''\n",
        "should print: ([[4.8, 3.1, 1.6, 1], [7.0, 3.2, 4.7, 0]], [[5.4, 3.4, 1.5, 1], [6.4, 3.6, 2.7, 0]])\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqlq1Mv1znlg"
      },
      "source": [
        "### 2.1.3 Find the best split (**6 points**){-}\n",
        "\n",
        "With the above `gini_score` and `create_split` functions, we now have everything needed to evaluate the splits.\n",
        "\n",
        "Given a list of data, we must check **every feature and every possible value of the feature in the datalist** as a candidate split, evaluate the cost of the split, and find the best possible split. Here we use the Gini index as the cost, and a lower value is better. Once the best split is found, we can use it as a node in our decision tree.\n",
        "\n",
        "**As an important note on the terminology**, both `internal node` and `terminal node` are collectively referred to as `node`, and are hence both subject to the maximum depth constraint.\n",
        "Both of them are different from `group`, which is just a list of examples.\n",
        "\n",
        "A terminal node will be directly represented by a class value (e.g., 0 or 1 as a floating point, or depending on how the dataset represents its labels).\n",
        "\n",
        "An internal/non-terminal node is represented by a dictionary of five fields:\n",
        "\n",
        "1. `index`: the index of the feature selected to split the node into two groups;\n",
        "\n",
        "2. `value`: the threshold of the feature by which the node is split;\n",
        "\n",
        "3. `groups`: the result of `create_split`, which encodes the left and right groups.  By running \"left_g, right_g = node['groups']\", one can retrieve the two groups.  Each group of data is its own small data list of just those examples assigned to the left or right group by the splitting process. \n",
        "\n",
        "4. `left`: a node that represents the left child.  It can be either a terminal node or an internal node.\n",
        "\n",
        "5. `right`: analogous to `left`.\n",
        "\n",
        "In the following code block, implement a function `get_best_split` to find the best split for the given data list. \n",
        "Return a dictionary (i.e., an internal node) whose `index`, `value`, and `groups` are populated, i.e., storing the index of the chosen feature, the chosen splitting threshold, and the resulting two groups.\n",
        "Leave `left` and `right` unspecified. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-QZtysb1vWt"
      },
      "source": [
        "def get_best_split(datalist):\n",
        "  '''\n",
        "  Inputs:\n",
        "  datalist: A list of samples. Each sample is a list, the last element is the label.\n",
        "  Outputs:\n",
        "  node: A dictionary contains 3 key value pairs, such as: node = {'index': integer, 'value': float, 'groups': a tuple contains two lists of examples}\n",
        "  Pseudo-code:\n",
        "  for index in range(#feature): # index is the feature index\n",
        "    for example in datalist:\n",
        "      use create_split with (index, example[index]) to divide datalist into two groups\n",
        "      compute the Gini index for this division\n",
        "  construct a node with the (index, example[index], groups) that corresponds to the lowest Gini index\n",
        "  '''\n",
        "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "  return node\n",
        "\n",
        "\n",
        "'''\n",
        "unit test:\n",
        "datalist = [[4.8, 3.1, 1.6, 0.3, 1],\n",
        "[5.4, 3.4, 1.5, 1.4, 1],\n",
        "[7.0, 3.2, 4.7, 1.4, 0],\n",
        "[6.4, 3.2, 2.7, 1.5, 0]]\n",
        "result = get_best_split(datalist)\n",
        "print(result)\n",
        "'''\n",
        "\n",
        "'''\n",
        "should print: \n",
        "{'index': 0, \n",
        " 'value': 6.4, \n",
        " 'groups': ([[4.8, 3.1, 1.6, 0.3, 1], [5.4, 3.4, 1.5, 1.4, 1]], [[7.0, 3.2, 4.7, 1.4, 0], [6.4, 3.2, 2.7, 1.5, 0]])}\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFQ6iGaN8klG"
      },
      "source": [
        "### 2.1.4 Build a tree {-}\n",
        "\n",
        "The construction of a decision tree consists of three major components:\n",
        "\n",
        "1. Recursive splitting.\n",
        "\n",
        "2. Termination condition of recursion: when to stop splitting\n",
        "\n",
        "3. Building a tree.\n",
        "\n",
        "**Step 1: termination function**\n",
        "\n",
        "For simplicity, we will start with the termination condition, and then move on to the recursion.  Two hyperparamters are important here:\n",
        "\n",
        "*   **Maximum Tree Depth**. This is the maximum number of predecessors (parent, grandparent, etc) that each node (terminal or internal/non-terminal) can have.  Once the maximum depth of the tree is met, we must stop splitting a node. Deeper trees are more complex and are more likely to overfit the training data.  Note we call the root node to have depth 1 (not 0), its child nodes with depth 2, and so on.\n",
        "\n",
        "*   **Minimum Node Size**. This is the minimum number of training examples that a node can contain. If a node is split into two groups and one of them falls below this minimum size, then we should stop splitting that group further, i.e., we should make that group a terminal node.  Nodes that account for too few training examples are expected to be too specific and are likely to overfit the training data.\n",
        "\n",
        "These two parameters will be specified by the user as input arguments of our tree building procedure.\n",
        "There is one more situation. It is possible that a split is chosen where all examples belong to one group, while the other group is empty. In this case, we will be unable to continue the splitting and should stop.\n",
        "Overall, there are 3 different **stopping conditions** which are clearly illustrated by the following pseudo-code (see the detailed explanations below).  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCZg7PvNOT-s"
      },
      "source": [
        "left_g, right_g = N['groups']\n",
        " \n",
        "if either left_g or right_g is empty:\n",
        "    Set both N['left'] and N['right'] to a terminal node encoding the most common label of the examples in N\n",
        "    return\n",
        " \n",
        "# check for max depth\n",
        "if depth of N >= max_depth - 1:   # use >= instead of == in case max_depth = 1\n",
        "    N['left'] = a terminal node encoding the most common label in left_g\n",
        "    N['right'] = a terminal node encoding the most common label in right_g\n",
        "    return\n",
        " \n",
        "# process left child\n",
        "if the number of examples in left_g <= min_size:\n",
        "    N['left'] = a terminal node encoding the most common label of in left_g\n",
        "else:\n",
        "    N['left'] = get_best_split(left_g)\n",
        "    build a tree on N['left'] (use recursion)\n",
        " \n",
        "# process right child similarly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2UqISgDPOJC"
      },
      "source": [
        "Here N is the current node which is represented by a dictionary specified in Section 2.1.3.  The best split has already been found for N using `get_best_split`, and the fields `index`, `value`, and `groups` have already been computed for N (but not yet for `left` and `right`). Note that upon the completion of running the pseudo-code, the fields `left` and `right` of N will have been populated, which correspond to the left and right child nodes, respectively.\n",
        "\n",
        "Note that we check \"if either left_g or right_g is empty\" before checking the max depth, because if either the left or the right group is empty, then N will become a terminal node returning its majority class, hence must satisfy the depth check. However, since we are already \"inside\" of N, we cannot change it into a terminal node per se; turning N into a terminal node can only be accomplished by N's own parent node (see the recursion in Step 2 below). As a result, we introduced a trick of adding a left child and a right child for N, both being a terminal node returning the majority class of N.  Strictly speaking, it is possible that the depth of N is already `max_depth`, which makes the left and right child nodes exceeding `max_depth`.  However, since this is just a workaround to represent that N is a terminal node, it does not make any difference in learning and prediction.\n",
        "\n",
        "In the following code block, implement a function `to_terminal` that returns the most common class value in a group.  This will be used to make predictions. (**4 points)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onjXZEZGrg21"
      },
      "source": [
        "def to_terminal(group):\n",
        "  '''\n",
        "  Input:\n",
        "    group: A list of examples. Each example is a list, whose last element is the label.\n",
        "  Output:\n",
        "    label: the label indicating the most common class value in the group\n",
        "  '''\n",
        "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "  return label\n",
        "\n",
        "'''\n",
        "unit test:\n",
        "group = [[4.8, 3.1, 1],\n",
        "[5.4, 3.4, 1],\n",
        "[7.0, 3.2, 0]]\n",
        "result = to_terminal(group)\n",
        "print(result)\n",
        "'''\n",
        "\n",
        "'''\n",
        "should print: 1\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyFblNhhrtEL"
      },
      "source": [
        "**Step 2: Recursive splitting**  (**15 points**)\n",
        "\n",
        "Building a decision tree involves calling the above functions `get_best_split` and `to_terminal` over and over again on the groups created for each node.\n",
        "Once a node is constructed, we can construct its child nodes recursively on each group of data from the split by calling the same node construction function again.\n",
        "\n",
        "In the following code block, implement a function `recursive_split` to conduct this recursive procedure. It takes a `node` as an argument as well as the maximum depth, minimum size in a node and the depth of `node`.\n",
        "Before invoking `recursive_split`, three key-value pairs `index`, `value`, and `groups` have already been computed for `node` using `get_best_split`, i.e., the best split has already been found and stored. \n",
        "\n",
        "In fact, all you need to do is to implement the pseudo-code in \"Step 1: termination function\". \n",
        "Here is the step-by-step explanation:\n",
        "\n",
        "1.   The two groups of data carried by the given `node` are extracted as left and right data lists for use. Then delete the `groups` field in `node` to save space because it will not be needed any more. This deletion is not reflected in the pseudo-code, and you need to implement it.\n",
        "\n",
        "2.   Next, we check if either the left group (`left_g`) or the right group (`right_g`) is empty.  If so, create a terminal node for both left and right groups by applying `to_terminal` on the non-empty group.  Yes, even for the empty group, we also adopt the result of `to_terminal` on the (other) non-empty group.  Think why.\n",
        "\n",
        "3.   We then check the depth. If `max_depth` is reached by the child node (i.e., the depth of `node` itself has reached `max_depth`-1), then  create terminal nodes for both the left and the right groups using `to_terminal`.\n",
        "\n",
        "4.   Then we process the left child.  If the left group's size is below `min_size`, then create a terminal node for it using `to_terminal`. Otherwise, create a split for the left group by `get_best_split`, and assign the resulting node to the `left` field of `node`. Then pass `node[left]` to the `recursive_split` function with 1 + the depth of `node`.\n",
        "\n",
        "5.   The right child is then processed in the same manner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMNimex8rprO"
      },
      "source": [
        "# Create child splits for a node or make terminal\n",
        "def recursive_split(node, max_depth, min_size, depth):\n",
        "  '''\n",
        "  Inputs:\n",
        "  node:  A dictionary contains 3 key value pairs, node = \n",
        "         {'index': integer, 'value': float, 'groups': a tuple contains two lists fo samples}\n",
        "  max_depth: maximum depth of the tree, an integer\n",
        "  min_size: minimum size of a group, an integer\n",
        "  depth: tree depth for current node\n",
        "  Output:\n",
        "  no need to output anything, the input node should carry its own subtree once this function ternimate\n",
        "  '''\n",
        "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "  # check for a no split\n",
        "\n",
        "  # check for max depth\n",
        "\n",
        "  # process left child\n",
        "\n",
        "  # process right child\n",
        "\n",
        "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "'''\n",
        "node = {'index': 2, 'value': 2.6, \n",
        "        'groups': ([[4.8, 3.4, 1.9, 0.2, 1], \n",
        "                    [6.0, 3.0, 1.6, 1.2, 1], \n",
        "                    [5.2, 3.5, 1.5, 0.6, 1], \n",
        "                    [4.8, 3.1, 1.6, 0.3, 1], \n",
        "                    [5.4, 3.4, 1.5, 1.4, 1], \n",
        "                    [4.3, 3.5, 1.6, 0.6, 0]], \n",
        "                   [[5.0, 3.4, 4.6, 1.9, 1], \n",
        "                    [5.2, 3.4, 3.4, 1.5, 1], \n",
        "                    [8.7, 3.2, 5.6, 0.2, 1], \n",
        "                    [7.0, 3.2, 4.7, 1.4, 0], \n",
        "                    [6.4, 3.2, 2.7, 1.5, 0], \n",
        "                    [4.9, 3.1, 4.9, 1.5, 0], \n",
        "                    [4.5, 2.3, 4.0, 0.3, 0], \n",
        "                    [6.5, 2.8, 2.6, 1.5, 0], \n",
        "                    [5.7, 3.8, 4.5, 1.3, 0], \n",
        "                    [4.9, 2.4, 3.3, 1.0, 0]])}\n",
        "max_depth = 4\n",
        "min_size = 3\n",
        "depth = 1\n",
        "recursive_split(node, max_depth, min_size, depth)\n",
        "print(node)\n",
        "'''\n",
        "\n",
        "'''\n",
        "should print:\n",
        "{'index': 2, \n",
        " 'value': 2.6, \n",
        " 'left': {'index': 0, \n",
        "          'value': 4.8, \n",
        "          'left': 0, \n",
        "          'right': {'index': 0, \n",
        "                    'value': 4.8, \n",
        "                    'left': 1, \n",
        "                    'right': 1}}, \n",
        "  'right': {'index': 1, \n",
        "            'value': 3.2, \n",
        "            'left': {'index': 0, \n",
        "                     'value': 4.9, \n",
        "                     'left': 0, \n",
        "                     'right': 0}, \n",
        "            'right': {'index': 0, \n",
        "                      'value': 5.7, \n",
        "                      'left': 1, \n",
        "                      'right': 0}}}\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QThrsoxWruLX"
      },
      "source": [
        "**Step 3: Build a tree** (**3 points**)\n",
        "\n",
        "We can now put all of the pieces together.\n",
        "In the following code block, implement a function `build_tree` to build a decision tree by the given training set, maximum depth, minimum size.\n",
        "We first create the root node by calling `get_best_split`. \n",
        "Then call `recursive_split` to build out the tree with the current depth set to 1.\n",
        "Upon completion of `recursive_split`, the root node should carry the whole tree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Go0tGfHa82v3"
      },
      "source": [
        "def build_tree(train, max_depth, min_size):\n",
        "  '''\n",
        "  Inputs:\n",
        "    - train: Training set, a list of examples. Each example is a list, whose last element is the label.\n",
        "    - max_depth: maximum depth of the tree, an integer (root has depth 1)\n",
        "    - min_size: minimum size of a group, an integer\n",
        "  Output:\n",
        "    - root: The root node, a recursive dictionary that should carry the whole tree\n",
        "  '''\n",
        "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "  return root\n",
        "\n",
        "'''\n",
        "unit test:\n",
        "train = [[4.8, 3.4, 1.9, 0.2, 1],\n",
        "[6.0, 3.0, 1.6, 1.2, 1],\n",
        "[5.0, 3.4, 4.6,\t1.9, 1],\n",
        "[5.2, 3.5, 1.5, 0.6, 1],\n",
        "[5.2, 3.4, 3.4, 1.5, 1],\n",
        "[8.7, 3.2, 5.6,\t0.2, 1],\n",
        "[4.8, 3.1, 1.6, 0.3, 1],\n",
        "[5.4, 3.4, 1.5, 1.4, 1],\n",
        "[7.0, 3.2, 4.7, 1.4, 0],\n",
        "[6.4, 3.2, 2.7, 1.5, 0],\n",
        "[4.9, 3.1, 4.9, 1.5, 0],\n",
        "[4.5, 2.3, 4.0, 0.3, 0],\n",
        "[6.5, 2.8, 2.6, 1.5, 0],\n",
        "[5.7, 3.8, 4.5, 1.3, 0],\n",
        "[4.3, 3.5, 1.6, 0.6, 0],\n",
        "[4.9, 2.4, 3.3, 1.0, 0]]\n",
        "max_depth = 4\n",
        "min_size = 3\n",
        "root = build_tree(train, max_depth, min_size)\n",
        "print(root)\n",
        "'''\n",
        "\n",
        "'''\n",
        "should print: \n",
        "{'index': 2, \n",
        " 'value': 2.6, \n",
        " 'left': {'index': 0, \n",
        "          'value': 4.8, \n",
        "          'left': 0, \n",
        "          'right': {'index': 0, \n",
        "                    'value': 4.8, \n",
        "                    'left': 1, \n",
        "                    'right': 1}}, \n",
        " 'right': {'index': 1, \n",
        "           'value': 3.2, \n",
        "           'left': {'index': 0, \n",
        "                    'value': 4.9, \n",
        "                    'left': 0, \n",
        "                    'right': 0}, \n",
        "           'right': {'index': 0, \n",
        "                     'value': 5.7, \n",
        "                     'left': 1, \n",
        "                     'right': 0}}}\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FndRt6CX86oo"
      },
      "source": [
        "### 2.1.5 Prediction (**6 points**){-}\n",
        "\n",
        "To make predictions with a decision tree, we need to navigate the tree for each test example.\n",
        "\n",
        "In the following code block, implement a function `predict` to predict the  label for all test examples.\n",
        "For each test example, the navigation of the tree can be implemented by using a while loop, or as a recursive function where the same prediction routine is called recursively based on the `index` and `value` fields of the internal nodes, until a terminal node is reached. \n",
        "\n",
        "In the function, we need to check if a child node is a terminal value (class label to be returned as the prediction), or an internal node that is represented as a dictionary. You may find the function [isinstance](https://www.w3schools.com/python/ref_func_isinstance.asp) useful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDaVD4s6UajU"
      },
      "source": [
        "# Make a prediction with a decision tree\n",
        "def predict(root, sample):\n",
        "  '''\n",
        "  Inputs:\n",
        "  root: the root node of the tree. a recursive dictionary that carries the whole tree.\n",
        "  sample: a list\n",
        "  Outputs:\n",
        "  '''\n",
        "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "'''\n",
        "unit test:\n",
        "root = {'index': 2, 'value': 2.6, 'left': {'index': 0, 'value': 4.8, 'left': 0, 'right': 1}, 'right': 1}\n",
        "sample1 = [5.4, 3.4, 1.5, 1.4]\n",
        "sample2 = [4.3, 3.5, 1.6, 0.6]\n",
        "print(predict(root, sample1))\n",
        "print(predict(root, sample2))\n",
        "'''\n",
        "'''\n",
        "should print: \n",
        "1\n",
        "0\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1vfVpLEUddM"
      },
      "source": [
        "## 2.2 Training and testing (**6 points**) {-}\n",
        "\n",
        "Now we are ready to apply this algorithm to the `Banknote` dataset. In the following code block, evaluate the decision tree model as follow:\n",
        "\n",
        "*   Load the dataset to a list of examples (already done)\n",
        "*   Since the loaded data features are of string type, covert all features to float type\n",
        "*   Split the dataset into a training set and a test set. Use the first 1000 samples for training, and the rest for testing.\n",
        "*   Build a tree by providing the traning set, maximum depth and minimum size.\n",
        "*   Make prediction for all test examples using the built tree.\n",
        "*   Print the accuracy and f1 score for the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXOtr-H8gsM_"
      },
      "source": [
        "# load and prepare data\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import shutil\n",
        "from csv import reader\n",
        "from random import seed\n",
        "\n",
        "url = 'https://www.cs.uic.edu/~zhangx/teaching/data_banknote_authentication.csv'\n",
        "file_name = 'data_banknote_authentication.csv'\n",
        "with urllib.request.urlopen(url) as response, open(file_name, 'wb') as out_file:\n",
        "    shutil.copyfileobj(response, out_file)\n",
        "\n",
        "file = open(file_name, \"rt\")\n",
        "lines = reader(file)\n",
        "\n",
        "df = pd.read_csv(file_name,\n",
        "                    sep='\\t',\n",
        "                    header=None)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIPftcqkX-V1"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "seed(1)\n",
        "\n",
        "dataset = list(lines)\n",
        "max_depth = 6\n",
        "min_size = 10\n",
        "num_train = 1000\n",
        "\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "# convert string attributes to integers\n",
        "\n",
        "\n",
        "# evaluate algorithm\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_si6Qa3wbuut"
      },
      "source": [
        "# Submission Instruction {-}\n",
        "\n",
        "You're almost done! Take the following steps to finally submit your work.\n",
        "\n",
        "1. After executing all commands and completing this notebook, save your `Lab_3.ipynb` as a PDF file, named as `X_Y_UIN.pdf`, where `X` is your first name, `Y` is your last name, and `UIN` is your UIN. Make sure that your PDF file includes all parts of your solution, including the plots. \n",
        "\n",
        "> * Print out all unit test case results before printing the notebook into a PDF.\n",
        "* If you use Colab, open this notebook in Chrome.  Then File -> Print -> set Destination to \"Save as PDF\".  If the web page freezes when printing, close Chrome and reopen the page. If Chrome doesn't work, try Firefox.\n",
        "* If you are working on your own computer, we recommend using the browser (not jupyter) for saving the PDF. For Chrome on a Mac, this is under *File->Print...->Open PDF in Preview*. When the PDF opens in Preview, you can use *Save...* to save it.\n",
        "* Sometimes, a figure that appears near the end of a page can get cut.  In this case, try to add some new lines in the preceding code block so that the figure is pushed to the beginning of the next page.\n",
        "\n",
        "2. Upload `X_Y_UIN.pdf` to Gradescope under `Lab_3_Written`.\n",
        "\n",
        "3. A template of `Lab_3.py` has been provided.  For all functions in `Lab_3.py`, copy the corresponding code snippets you have written into it.  Do not copy any code of plotting figures. **Do NOT** change the function names.\n",
        "\n",
        "4. Zip `Lab_3.py` and `Lab_3.ipynb` (**2 files**) into a zip file named `X_Y_UIN.zip`. Suppose the two files are in the folder `Lab_3`.  Then zip up the two files inside the `Lab_3` folder.  Do NOT zip up the folder `Lab_3`. Submit this zip file to Gradescope under `Lab_3_Code`. \n",
        "\n",
        "5. Only one member of each team needs to submit.  But please specify **all** your teammates on Gradescope. After each submission, please click 'Add Group Member' (top-right corner of your submission page) to claim all teammates.\n",
        "\n",
        "You can submit to Gradescope as often as you like. We will only consider your last submission before the deadline."
      ]
    }
  ]
}